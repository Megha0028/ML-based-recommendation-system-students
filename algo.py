# -*- coding: utf-8 -*-
"""dsfinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S4HxgOXZ-1dzyBJSfA5zp9ZeGxhoIZ9j
"""

# %% [code]
# Install required packages
!pip install tensorflow xgboost shap scikit-optimize

# %% [code]
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import xgboost as xgb
import shap
from skopt import BayesSearchCV
from skopt.space import Real, Categorical, Integer

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# %% [code]
# Load dataset
file_path = '/content/drive/MyDrive/personalized_learning_dataset.csv'  # Update with your path
df = pd.read_csv(file_path)  # Assuming space-separated values

# Display basic info
print(df.info())
print(df.head())

# %% [code]
# Data Preprocessing
# Convert categorical variables
df['Dropout_Likelihood'] = df['Dropout_Likelihood'].map({'Yes': 1, 'No': 0})

# Feature Engineering
# Create interaction features
df['Engagement_Score'] = df['Time_Spent_on_Videos'] * df['Quiz_Attempts']
df['Learning_Efficiency'] = df['Quiz_Scores'] / (df['Time_Spent_on_Videos'] + 1)
df['Activity_Balance'] = df['Forum_Participation'] * df['Assignment_Completion_Rate']

# Encode categorical variables
label_encoders = {}
categorical_cols = ['Gender', 'Education_Level', 'Course_Name', 'Engagement_Level', 'Learning_Style']
for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# %% [code]
# Define features and target
features = ['Age', 'Gender', 'Education_Level', 'Course_Name', 'Time_Spent_on_Videos',
            'Quiz_Attempts', 'Quiz_Scores', 'Forum_Participation',
            'Assignment_Completion_Rate', 'Engagement_Level', 'Learning_Style',
            'Feedback_Score', 'Engagement_Score', 'Learning_Efficiency', 'Activity_Balance']

target = 'Dropout_Likelihood'

X = df[features]
y = df[target]

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# %% [code]
# Wide & Deep Learning Model
def create_wide_deep_model():
    # Define input layers
    numeric_inputs = Input(shape=(len(features)-len(categorical_cols),), name='numeric_inputs')

    # Create embedding layers for categorical features
    categorical_inputs = []
    embedding_layers = []

    for col in categorical_cols:
        # Find the column index
        idx = features.index(col)
        # Get number of unique values
        n_unique = df[col].nunique()
        # Create input and embedding layer
        inp = Input(shape=(1,), name=f'{col}_input')
        emb = Embedding(input_dim=n_unique+1, output_dim=min(10, (n_unique+1)//2), name=f'{col}_embedding')(inp)
        flat = Flatten()(emb)
        categorical_inputs.append(inp)
        embedding_layers.append(flat)

    # Wide part (linear model with cross features)
    wide = Concatenate()([numeric_inputs] + embedding_layers)
    wide = Dense(32, activation='relu')(wide)

    # Deep part
    deep = Concatenate()([numeric_inputs] + embedding_layers)
    deep = Dense(128, activation='relu')(deep)
    deep = BatchNormalization()(deep)
    deep = Dropout(0.3)(deep)
    deep = Dense(64, activation='relu')(deep)
    deep = BatchNormalization()(deep)
    deep = Dropout(0.2)(deep)
    deep = Dense(32, activation='relu')(deep)

    # Combine wide and deep
    combined = Concatenate()([wide, deep])
    output = Dense(1, activation='sigmoid')(combined)

    # Define model
    inputs = [numeric_inputs] + categorical_inputs
    model = Model(inputs=inputs, outputs=output)

    model.compile(optimizer=Adam(learning_rate=0.001),
                  loss='binary_crossentropy',
                  metrics=['accuracy', tf.keras.metrics.AUC(),tf.keras.metrics.Recall()],
                   weighted_metrics=['accuracy'])

    return model

# %% [code]
# Prepare data for Wide & Deep model
# Separate numeric and categorical features
numeric_features = [f for f in features if f not in categorical_cols]
numeric_idx = [features.index(f) for f in numeric_features]
categorical_idx = [features.index(f) for f in categorical_cols]

# Scale numeric features
scaler = StandardScaler()
X_train_num = scaler.fit_transform(X_train.iloc[:, numeric_idx])
X_test_num = scaler.transform(X_test.iloc[:, numeric_idx])

# Prepare categorical features
X_train_cat = [X_train.iloc[:, idx].values for idx in categorical_idx]
X_test_cat = [X_test.iloc[:, idx].values for idx in categorical_idx]

# Prepare input data
X_train_wd = [X_train_num] + X_train_cat
X_test_wd = [X_test_num] + X_test_cat

# %% [code]
# Train Wide & Deep model
wd_model = create_wide_deep_model()
#early_stopping = EarlyStopping(patience=50, restore_best_weights=True)

history = wd_model.fit(X_train_wd, y_train,
                      epochs=100,
                      batch_size=64,
                      validation_split=0.2,
                      #callbacks=[early_stopping],
                        class_weight={0: 1, 1: 6},
                      verbose=1)

# Evaluate
wd_pred = (wd_model.predict(X_test_wd) > 0.5).astype(int)
print("Wide & Deep Model Performance:")
print(classification_report(y_test, wd_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, wd_pred))

# %% [code]
# XGBoost with Bayesian Optimization
# Define search space
params = {
    'learning_rate': Real(0.01, 0.3, prior='log-uniform'),
    'max_depth': Integer(3, 10),
    'min_child_weight': Integer(1, 10),
    'subsample': Real(0.5, 1.0),
    'colsample_bytree': Real(0.5, 1.0),
    'gamma': Real(0, 5),
    'n_estimators': Integer(50, 300)
}

# Create XGBoost classifier
xgb_clf = xgb.XGBClassifier(objective='binary:logistic', eval_metric='auc', use_label_encoder=False,scale_pos_weight=5)

# Bayesian optimization
opt = BayesSearchCV(
    xgb_clf,
    params,
    n_iter=50,
    cv=5,
    n_jobs=-1,
    random_state=42
)

opt.fit(X_train, y_train)

# Best model
best_xgb = opt.best_estimator_
xgb_pred = best_xgb.predict(X_test)

print("\nOptimized XGBoost Performance:")
print(classification_report(y_test, xgb_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, xgb_pred))

# %% [code]
# Ensemble Model (Average predictions)
wd_probs = wd_model.predict(X_test_wd).flatten()
xgb_probs = best_xgb.predict_proba(X_test)[:, 1]

# Weighted average (you can tune these weights)
ensemble_probs = 0.6 * wd_probs + 0.4 * xgb_probs
ensemble_pred = (ensemble_probs > 0.5).astype(int)

print("\nEnsemble Model Performance:")
print(classification_report(y_test, ensemble_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, ensemble_pred))

# %% [code]
# SHAP Analysis for Explainability
explainer = shap.Explainer(best_xgb)
shap_values = explainer(X_train)

# Summary plot
plt.figure(figsize=(10, 8))
shap.summary_plot(shap_values, X_train, feature_names=features)
plt.title('Feature Importance for Dropout Prediction')
plt.tight_layout()
plt.show()

# %% [code]
# Recommendation System

def generate_recommendations(student_data, model, features, categorical_cols, label_encoders):
    """Generate personalized recommendations for a student"""

    # Convert input to DataFrame
    student_df = pd.DataFrame([student_data])

    # Calculate derived features (same as we did for training)
    student_df['Engagement_Score'] = student_df['Time_Spent_on_Videos'] * student_df['Quiz_Attempts']
    student_df['Learning_Efficiency'] = student_df['Quiz_Scores'] / (student_df['Time_Spent_on_Videos'] + 1)
    student_df['Activity_Balance'] = student_df['Forum_Participation'] * student_df['Assignment_Completion_Rate']

    for col in categorical_cols:
        if col in student_df.columns:  # Only encode if present in input
            le = label_encoders[col]
            # Check if the value is in the classes of the LabelEncoder
            if student_df[col][0] in le.classes_:
                student_df[col] = le.transform(student_df[col])
            else:
                # Handle unseen labels by assigning a default numerical value
                # instead of trying to concatenate with a string
                student_df[col] = len(le.classes_)  # Assign a new numerical label
                # Update the LabelEncoder's classes_ to include the new label
                le.classes_ = np.append(le.classes_, student_df[col][0])


    # Prepare data for model
    available_features = [f for f in features if f in student_df.columns]
    X_student = student_df[available_features]

    # Get prediction
    if model == 'xgb':
        proba = best_xgb.predict_proba(X_student)[0][1]
    else:  # wide & deep
        numeric_features = [f for f in available_features if f not in categorical_cols]
        X_num = scaler.transform(student_df[numeric_features])
        X_cat = [student_df[col].values for col in categorical_cols if col in student_df.columns]
        X_wd = [X_num] + X_cat
        proba = wd_model.predict(X_wd)[0][0]

    # Generate recommendations based on risk factors
    recommendations = []
    student_series = student_df.iloc[0]  # Get first row as Series

    if proba > 0.5:  # High dropout risk
        if student_series['Forum_Participation'] < 10:
            recommendations.append("Increase forum participation to at least 10 posts per course")
        if student_series['Quiz_Attempts'] < 3:
            recommendations.append("Attempt quizzes at least 3 times to reinforce learning")
        if student_series['Time_Spent_on_Videos'] < 200:
            recommendations.append("Spend more time on course videos (recommended >200 minutes)")
        if student_series['Assignment_Completion_Rate'] < 80:
            recommendations.append("Focus on completing assignments (target >80% completion rate)")
    else:  # Low risk
        recommendations.append("Your current engagement level is good. Maintain these habits.")
        if student_series['Quiz_Scores'] < 70:
            recommendations.append("Consider reviewing quiz materials to improve scores")
        if student_series['Learning_Efficiency'] < 0.3:
            recommendations.append("Try more focused study sessions to improve learning efficiency")

    # Learning style specific recommendations
    if 'Learning_Style' in student_series:
        learning_style_val = int(student_series['Learning_Style'])  # Convert to int
        learning_style = label_encoders['Learning_Style'].inverse_transform([learning_style_val])[0]
        if learning_style == 'Visual':
            recommendations.append("Try using more diagrams and charts in your study materials")
        elif learning_style == 'Auditory':
            recommendations.append("Consider recording lectures or using audio materials")
        elif learning_style == 'Reading/Writing':
            recommendations.append("Take detailed notes and rewrite concepts in your own words")
        elif learning_style == 'Kinesthetic':
            recommendations.append("Use hands-on activities or practical exercises to reinforce concepts")

    return {
        'dropout_risk': float(proba),
        'recommendations': recommendations
    }

# Example usage with properly encoded categorical variables
sample_student = {
    'Age': 25,
    'Gender': 'Female',
    'Education_Level': 'Undergraduate',
    'Course_Name': 'Machine Learning',
    'Time_Spent_on_Videos': 150,
    'Quiz_Attempts': 2,
    'Quiz_Scores': 60,
    'Forum_Participation': 5,
    'Assignment_Completion_Rate': 75,
    'Engagement_Level': 'Medium',
    'Learning_Style': 'Visual',
    'Feedback_Score': 3
}

# Pre-encode categorical variables to match training data
for col in categorical_cols:
    if col in sample_student and isinstance(sample_student[col], str):
        le = label_encoders[col]
        sample_student[col] = le.transform([sample_student[col]])[0]

recs = generate_recommendations(sample_student, 'xgb', features, categorical_cols, label_encoders)
print("\nPersonalized Recommendations:")
for i, rec in enumerate(recs['recommendations'], 1):
    print(f"{i}. {rec}")
print(f"\nPredicted Dropout Risk: {recs['dropout_risk']:.2%}")

import joblib
import tensorflow as tf

# ... (Your existing code) ...

# Save models and label encoders
wd_model.save('wide_deep_model.h5')  # Save Wide & Deep model
joblib.dump(best_xgb, 'xgboost_model.pkl')  # Save XGBoost model
joblib.dump(label_encoders, 'label_encoders.pkl')  # Save label encoders
joblib.dump(scaler, 'scaler.pkl') # Save scaler

# ... (Later, to load and use the models) ...

# Load models and label encoders
wd_model = tf.keras.models.load_model('wide_deep_model.h5')
best_xgb = joblib.load('xgboost_model.pkl')
label_encoders = joblib.load('label_encoders.pkl')
scaler = joblib.load('scaler.pkl')

# ... (Use the models to generate recommendations) ...

recs = generate_recommendations(sample_student, 'xgb', features, categorical_cols, label_encoders)
print("\nPersonalized Recommendations:")
for i, rec in enumerate(recs['recommendations'], 1):
    print(f"{i}. {rec}")
print(f"\nPredicted Dropout Risk: {recs['dropout_risk']:.2%}")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc
# ... (rest of your imports) ...

# ... (Your existing code for data loading, preprocessing, model training, etc.) ...

# Visualizations

# 1. Wide & Deep Model Training History
plt.figure(figsize=(10, 5))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Wide & Deep Model Training History')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# 2. Confusion Matrix (for Wide & Deep and XGBoost)
def plot_confusion_matrix(y_true, y_pred, model_name):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix - {model_name}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

plot_confusion_matrix(y_test, wd_pred, 'Wide & Deep')
plot_confusion_matrix(y_test, xgb_pred, 'XGBoost')

# 3. ROC Curve (for Wide & Deep and XGBoost)
def plot_roc_curve(y_true, y_probs, model_name):
    fpr, tpr, _ = roc_curve(y_true, y_probs)
    roc_auc = auc(fpr, tpr)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'{model_name} (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'Receiver Operating Characteristic (ROC) Curve - {model_name}')
    plt.legend(loc="lower right")
    plt.show()

plot_roc_curve(y_test, wd_probs, 'Wide & Deep')
plot_roc_curve(y_test, xgb_probs, 'XGBoost')

# ... (Your existing code for recommendations, etc.) ...

!pip install visualkeras

!pip install visualkeras # Install visualkeras if you haven't already
import visualkeras
from tensorflow.keras.utils import plot_model

# Assuming 'wd_model' is your Wide & Deep model
visualkeras.layered_view(wd_model, legend=True, to_file='wide_deep_model.png')  # Display and save as PNG
plot_model(wd_model, to_file='wide_deep_model_plot.png', show_shapes=True, show_layer_names=True) # alternative with plot_model

# Assuming 'wd_model' is your Wide & Deep model
for layer in wd_model.layers:
    print(layer.name, layer.output_shape)

